{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-path",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend path added: d:\\My Data\\Rag\\rag-project01-framework\\backend\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 添加 backend 路径\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    backend_path = os.path.abspath(os.path.join(current_dir, '..', 'backend'))\n",
    "    env_path = os.path.abspath(os.path.join(current_dir, '..', 'backend', '.env'))\n",
    "else:\n",
    "    backend_path = os.path.abspath(os.path.join(current_dir, 'backend'))\n",
    "    env_path = os.path.abspath(os.path.join(current_dir, 'backend', '.env'))\n",
    "\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.append(backend_path)\n",
    "\n",
    "load_dotenv(env_path)\n",
    "print(f\"Backend path added: {backend_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "init-services",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Data\\Rag\\rag-project01-framework\\.venv\\lib\\site-packages\\pymilvus\\client\\__init__.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Services initialized.\n"
     ]
    }
   ],
   "source": [
    "from services.vector_store_service import VectorStoreService, VectorDBConfig\n",
    "from services.search_service import SearchService\n",
    "from utils.config import VectorDBProvider\n",
    "import os\n",
    "\n",
    "vector_store_service = VectorStoreService()\n",
    "search_service = SearchService()\n",
    "print(\"Services initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "list-collections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Chroma collections: ['074_ollama_20251217153128', '074_ollama_20251217153318', '074_ollama_20251217151221', '074_bedrock_20251217150304', 'DeepSeek_R1_jishubaogaozhongwenban_ollama_20251217155428', '074_ollama_20251217152903', '074_ollama_20251217152717', 'DeepSeek_R1_jishubaogaozhongwenban_ollama_20251217154928']\n"
     ]
    }
   ],
   "source": [
    "# 列出当前的 Chroma 集合\n",
    "collections = vector_store_service.list_collections(provider=VectorDBProvider.CHROMA)\n",
    "print(f\"Current Chroma collections: {collections}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "index-document",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using specified file: DeepSeek-R1-技术报告中文版_huggingface_20250312225906.json\n",
      "Re-embedding content using local Ollama model (to ensure dimension match)...\n",
      "Generating new embeddings for 5 chunks using nomic-embed-text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Data\\Rag\\rag-project01-framework\\backend\\services\\embedding_service.py:335: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  return OllamaEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created re-embedded temp file: temp_ollama_reembedded.json\n",
      "Indexing file: temp_ollama_reembedded.json\n",
      "Indexing result: {'database': <VectorDBProvider.CHROMA: 'chroma'>, 'index_mode': 'flat', 'total_vectors': 5, 'index_size': 5, 'processing_time': 0.062643, 'collection_name': 'DeepSeek_R1_jishubaogaozhongwenban_ollama_20251217155922'}\n"
     ]
    }
   ],
   "source": [
    "# 选择一个现有的 embedded 文档进行索引\n",
    "# 这里我们查找 backend/02-embedded-docs 目录下的第一个 json 文件\n",
    "embedded_docs_dir = os.path.join(backend_path, \"02-embedded-docs\")\n",
    "embedded_files = [f for f in os.listdir(embedded_docs_dir) if f.endswith('.json')]\n",
    "\n",
    "if not embedded_files:\n",
    "    print(\"No embedded documents found. Please run homework_1_embedding.ipynb first to generate embeddings.\")\n",
    "else:\n",
    "    # 指定要索引的目标文件\n",
    "    target_filename = \"DeepSeek-R1-技术报告中文版_huggingface_20250312225906.json\"\n",
    "    file_path = os.path.join(embedded_docs_dir, target_filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Using specified file: {target_filename}\")\n",
    "        \n",
    "        import json\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        print(\"Re-embedding content using local Ollama model (to ensure dimension match)...\")\n",
    "        # 提取文本内容\n",
    "        chunks = []\n",
    "        for item in data[\"embeddings\"]:\n",
    "            # 构造原始 chunk 结构\n",
    "            chunk = {\n",
    "                \"content\": item[\"metadata\"][\"content\"],\n",
    "                \"metadata\": {\n",
    "                    \"chunk_id\": item[\"metadata\"][\"chunk_id\"],\n",
    "                    \"page_number\": item[\"metadata\"][\"page_number\"],\n",
    "                    \"page_range\": item[\"metadata\"][\"page_range\"],\n",
    "                    \"word_count\": item[\"metadata\"][\"word_count\"]\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "        # 重新生成 Embedding\n",
    "        from services.embedding_service import EmbeddingService, EmbeddingConfig, EmbeddingProvider\n",
    "        from datetime import datetime\n",
    "        \n",
    "        # 临时实例化一个服务来处理生成\n",
    "        temp_embedding_service = EmbeddingService()\n",
    "        \n",
    "        local_model = \"nomic-embed-text\"\n",
    "        config = EmbeddingConfig(\n",
    "            provider=EmbeddingProvider.OLLAMA,\n",
    "            model_name=local_model\n",
    "        )\n",
    "        \n",
    "        input_data = {\n",
    "            \"chunks\": chunks,\n",
    "            \"metadata\": {\"filename\": data.get(\"filename\", \"unknown.pdf\")}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"Generating new embeddings for {len(chunks)} chunks using {local_model}...\")\n",
    "            new_embeddings, _ = temp_embedding_service.create_embeddings(input_data, config)\n",
    "            \n",
    "            # 保存到临时文件\n",
    "            import tempfile\n",
    "            temp_dir = os.path.dirname(file_path)\n",
    "            temp_file_path = os.path.join(temp_dir, \"temp_ollama_reembedded.json\")\n",
    "            \n",
    "            # 构造符合 save_embeddings 输出格式的数据\n",
    "            output_data = {\n",
    "                \"filename\": data.get(\"filename\", \"unknown.pdf\"),\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"embedding_provider\": \"ollama\",\n",
    "                \"embedding_model\": local_model,\n",
    "                \"vector_dimension\": len(new_embeddings[0][\"embedding\"]),\n",
    "                \"embeddings\": new_embeddings\n",
    "            }\n",
    "            \n",
    "            with open(temp_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "            target_file = \"temp_ollama_reembedded.json\"\n",
    "            file_path = temp_file_path\n",
    "            print(f\"Created re-embedded temp file: {target_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Re-embedding failed: {e}\")\n",
    "            print(\"Please ensure Ollama is running and model is pulled.\")\n",
    "            raise e\n",
    "        \n",
    "    else:\n",
    "        print(f\"Warning: Specified file {target_filename} not found!\")\n",
    "        # Fallback logic\n",
    "        if embedded_files:\n",
    "             target_file = embedded_files[0]\n",
    "             file_path = os.path.join(embedded_docs_dir, target_file)\n",
    "             print(f\"Falling back to first available file: {target_file}\")\n",
    "    \n",
    "    print(f\"Indexing file: {target_file}\")\n",
    "    \n",
    "    # 配置使用 Chroma\n",
    "    config = VectorDBConfig(\n",
    "        provider=VectorDBProvider.CHROMA,\n",
    "        index_mode=\"flat\"  # Chroma 默认索引，这里参数可能不直接影响 Chroma 但需要保持接口一致\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = vector_store_service.index_embeddings(file_path, config)\n",
    "        print(\"Indexing result:\", result)\n",
    "        indexed_collection_name = result[\"collection_name\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Indexing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search-chroma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'DeepSeek' in collection 'DeepSeek_R1_jishubaogaozhongwenban_ollama_20251217155922' using Chroma...\n",
      "Search Results (Threshold=0.1):\n",
      "- Score: 0.5666\n",
      "  Text: • 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n",
      "要等。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-na...\n",
      "  Metadata: {'source': 'DeepSeek-R1-技术报告中文版.pdf', 'page': '5', 'chunk': 5, 'total_chunks': 5, 'page_range': '5', 'embedding_provider': 'ollama', 'embedding_model': 'nomic-embed-text', 'embedding_timestamp': '2025-12-17T15:59:22.229439'}\n",
      "- Score: 0.5505\n",
      "  Text: 1.1. 贡献\n",
      "训练后：在基础模型上进行大规模强化学习\n",
      "• 我们直接将强化学习（ RL ）应用于基础模型，而不依赖于监督微调（ SFT ）作为初步步骤\n",
      "。这种方法使模型能够探索思维链（ CoT ）以解...\n",
      "  Metadata: {'source': 'DeepSeek-R1-技术报告中文版.pdf', 'page': '4', 'chunk': 4, 'total_chunks': 5, 'page_range': '4', 'embedding_provider': 'ollama', 'embedding_model': 'nomic-embed-text', 'embedding_timestamp': '2025-12-17T15:59:19.141814'}\n",
      "- Score: 0.5222\n",
      "  Text: 目录\n",
      "1 引言 3\n",
      "1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001...\n",
      "  Metadata: {'source': 'DeepSeek-R1-技术报告中文版.pdf', 'page': '2', 'chunk': 2, 'total_chunks': 5, 'page_range': '2', 'embedding_provider': 'ollama', 'embedding_model': 'nomic-embed-text', 'embedding_timestamp': '2025-12-17T15:59:12.329408'}\n"
     ]
    }
   ],
   "source": [
    "# 使用 Chroma 进行搜索\n",
    "if 'indexed_collection_name' in locals():\n",
    "    query = \"DeepSeek\"\n",
    "    print(f\"Searching for '{query}' in collection '{indexed_collection_name}' using Chroma...\")\n",
    "    \n",
    "    try:\n",
    "        results = await search_service.search(\n",
    "            query=query,\n",
    "            collection_id=indexed_collection_name,\n",
    "            provider=VectorDBProvider.CHROMA,\n",
    "            top_k=3,\n",
    "            threshold=0.3 \n",
    "        )\n",
    "        \n",
    "        print(f\"Search Results (Threshold=0.1):\")\n",
    "        if not results[\"results\"]:\n",
    "             print(\"No results found. The similarity score might be too low.\")\n",
    "             \n",
    "        for res in results[\"results\"]:\n",
    "            print(f\"- Score: {res['score']:.4f}\")\n",
    "            print(f\"  Text: {res['text'][:100]}...\")\n",
    "            print(f\"  Metadata: {res['metadata']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Skipping search as indexing failed or no collection created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
